{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stf4041\\Documents\\DownloadedSoftware\\envs\\py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from time import time\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras import callbacks\n",
    "from keras.initializers import VarianceScaling\n",
    "from sklearn.cluster import KMeans\n",
    "#import metrics\n",
    "from sklearn import manifold\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Input, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape, Conv2DTranspose\n",
    "import time\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from keras import regularizers\n",
    "from keras.layers import Lambda\n",
    "import tensorflow as tf\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "import metrics\n",
    "from keras.models import Sequential, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "#Load dataset for convolutional input\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x = np.concatenate((x_train, x_test))\n",
    "y = np.concatenate((y_train, y_test))\n",
    "x = x.reshape(x.shape + (1,))\n",
    "print (x.shape)\n",
    "x = np.divide(x, 255.) #reduced to 1000 samples to allow the dbscan to run\n",
    "#print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight((self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def antirectifier(x):\n",
    "    #x -= K.mean(x, axis=1, keepdims=True)\n",
    "    x = K.l2_normalize(x, axis=1)\n",
    "    return x\n",
    "\n",
    "def CAE(input_shape=(28, 28, 1), filters=[32, 64, 128, 10]):\n",
    "    norm = Lambda(lambda y: K.l2_normalize(y, axis=1))\n",
    "    model = Sequential()\n",
    "    if input_shape[0] % 8 == 0:\n",
    "        pad3 = 'same'\n",
    "    else:\n",
    "        pad3 = 'valid'\n",
    "    model.add(Conv2D(filters[0], 5, strides=2, padding='same', activation='relu', name='conv1', input_shape=input_shape))\n",
    "\n",
    "    model.add(Conv2D(filters[1], 5, strides=2, padding='same', activation='relu', name='conv2'))\n",
    "\n",
    "    model.add(Conv2D(filters[2], 3, strides=2, padding=pad3, activation='relu', name='conv3'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=filters[3], name='embedding'))\n",
    "    model.add(Lambda(antirectifier))\n",
    "    model.add(Dense(units=filters[2]*int(input_shape[0]/8)*int(input_shape[0]/8), activation='relu'))\n",
    "\n",
    "    model.add(Reshape((int(input_shape[0]/8), int(input_shape[0]/8), filters[2])))\n",
    "    model.add(Conv2DTranspose(filters[1], 3, strides=2, padding=pad3, activation='relu', name='deconv3'))\n",
    "\n",
    "    model.add(Conv2DTranspose(filters[0], 5, strides=2, padding='same', activation='relu', name='deconv2'))\n",
    "\n",
    "    model.add(Conv2DTranspose(input_shape[2], 5, strides=2, padding='same', name='deconv1'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCEC(object):\n",
    "    def __init__(self,\n",
    "                 input_shape,\n",
    "                 filters=[32, 64, 128, 10],\n",
    "                 n_clusters=10,\n",
    "                 alpha=1.0):\n",
    "\n",
    "        super(DCEC, self).__init__()\n",
    "\n",
    "        self.n_clusters = n_clusters\n",
    "        self.input_shape = input_shape\n",
    "        self.alpha = alpha\n",
    "        self.pretrained = False\n",
    "        self.y_pred = []\n",
    "\n",
    "        self.cae = CAE(input_shape, filters)\n",
    "        hidden = self.cae.get_layer(name='embedding').output\n",
    "        self.encoder = Model(inputs=self.cae.input, outputs=hidden)\n",
    "\n",
    "        # Define DCEC model\n",
    "        clustering_layer = ClusteringLayer(self.n_clusters, name='clustering')(hidden)\n",
    "        self.model = Model(inputs=self.cae.input,\n",
    "                           outputs=[clustering_layer, self.cae.output])\n",
    "\n",
    "    def pretrain(self, x, batch_size=256, epochs=200, optimizer='adam', save_dir='results/temp'):\n",
    "        print('...Pretraining...')\n",
    "        self.cae.compile(optimizer=optimizer, loss='mse')\n",
    "        from keras.callbacks import CSVLogger\n",
    "        csv_logger = CSVLogger(save_dir + '/pretrain_log.csv')\n",
    "\n",
    "        # begin training\n",
    "        #t0 = time()\n",
    "        self.cae.fit(x, x, batch_size=batch_size, epochs=epochs, callbacks=[csv_logger])\n",
    "        #print('Pretraining time: ', time() - t0)\n",
    "        self.cae.save(save_dir + '/pretrain_cae_model.h5')\n",
    "        print('Pretrained weights are saved to %s/pretrain_cae_model.h5' % save_dir)\n",
    "        self.pretrained = True\n",
    "\n",
    "    def load_weights(self, weights_path):\n",
    "        self.model.load_weights(weights_path)\n",
    "\n",
    "    def extract_feature(self, x):  # extract features from before clustering layer\n",
    "        return self.encoder.predict(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        q, _ = self.model.predict(x, verbose=0)\n",
    "        return q.argmax(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def target_distribution(q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "\n",
    "    def compile(self, loss=['kld', 'mse'], loss_weights=[1, 1], optimizer='adam'):\n",
    "        self.model.compile(loss=loss, loss_weights=loss_weights, optimizer=optimizer)\n",
    "\n",
    "    def fit(self, x, y=None, batch_size=256, maxiter=2e4, tol=1e-3,\n",
    "            update_interval=140, cae_weights=None, save_dir='./results/temp'):\n",
    "        \n",
    "        print('Update interval', update_interval)\n",
    "        save_interval = x.shape[0] / batch_size * 5\n",
    "        print('Save interval', save_interval)\n",
    "\n",
    "        # Step 1: pretrain if necessary\n",
    "        #t0 = time()\n",
    "        if not self.pretrained and cae_weights is None:\n",
    "            print('...pretraining CAE using default hyper-parameters:')\n",
    "            print('   optimizer=\\'adam\\';   epochs=200')\n",
    "            self.pretrain(x, batch_size, save_dir=save_dir)\n",
    "            self.pretrained = True\n",
    "        elif cae_weights is not None:\n",
    "            self.cae.load_weights(cae_weights)\n",
    "            print('cae_weights is loaded successfully.')\n",
    "\n",
    "        # Step 2: initialize cluster centers using k-means\n",
    "        #t1 = time()\n",
    "        print('Initializing cluster centers with k-means.')\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n",
    "        self.y_pred = kmeans.fit_predict(self.encoder.predict(x))\n",
    "        y_pred_last = np.copy(self.y_pred)\n",
    "        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "        \n",
    "        \n",
    "        # Step 3: deep clustering\n",
    "        # logging file\n",
    "        import csv, os\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        logfile = open(save_dir + '/dcec_log.csv', 'w')\n",
    "        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'L', 'Lc', 'Lr'])\n",
    "        logwriter.writeheader()\n",
    "\n",
    "        #t2 = time()\n",
    "        loss = [0, 0, 0]\n",
    "        index = 0\n",
    "        for ite in range(int(maxiter)):\n",
    "            if ite % update_interval == 0:\n",
    "                q, _ = self.model.predict(x, verbose=0)\n",
    "                p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "                # evaluate the clustering performance\n",
    "                self.y_pred = q.argmax(1)\n",
    "                if y is not None:\n",
    "                    acc = np.round(metrics.acc(y, self.y_pred), 5)\n",
    "                    nmi = np.round(metrics.nmi(y, self.y_pred), 5)\n",
    "                    ari = np.round(metrics.ari(y, self.y_pred), 5)\n",
    "                    loss = np.round(loss, 5)\n",
    "                    logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, L=loss[0], Lc=loss[1], Lr=loss[2])\n",
    "                    logwriter.writerow(logdict)\n",
    "                    print('Iter', ite, ': Acc', acc, ', nmi', nmi, ', ari', ari, '; loss=', loss)\n",
    "\n",
    "                # check stop criterion\n",
    "                delta_label = np.sum(self.y_pred != y_pred_last).astype(np.float32) / self.y_pred.shape[0]\n",
    "                y_pred_last = np.copy(self.y_pred)\n",
    "                if ite > 0 and delta_label < tol:\n",
    "                    print('delta_label ', delta_label, '< tol ', tol)\n",
    "                    print('Reached tolerance threshold. Stopping training.')\n",
    "                    logfile.close()\n",
    "                    break\n",
    "\n",
    "            # train on batch\n",
    "            if (index + 1) * batch_size > x.shape[0]:\n",
    "                loss = self.model.train_on_batch(x=x[index * batch_size::],\n",
    "                                                 y=[p[index * batch_size::], x[index * batch_size::]])\n",
    "                index = 0\n",
    "            else:\n",
    "                loss = self.model.train_on_batch(x=x[index * batch_size:(index + 1) * batch_size],\n",
    "                                                 y=[p[index * batch_size:(index + 1) * batch_size],\n",
    "                                                    x[index * batch_size:(index + 1) * batch_size]])\n",
    "                index += 1\n",
    "\n",
    "            # save intermediate model\n",
    "            if ite % save_interval == 0:\n",
    "                # save DCEC model checkpoints\n",
    "                print('saving model to:', save_dir + '/dcec_model_' + str(ite) + '.h5')\n",
    "                self.model.save_weights(save_dir + '/dcec_model_' + str(ite) + '.h5')\n",
    "\n",
    "            ite += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv2D)               (None, 14, 14, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 7, 7, 64)          51264     \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            (None, 10)                11530     \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1152)              12672     \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "deconv3 (Conv2DTranspose)    (None, 7, 7, 64)          73792     \n",
      "_________________________________________________________________\n",
      "deconv2 (Conv2DTranspose)    (None, 14, 14, 32)        51232     \n",
      "_________________________________________________________________\n",
      "deconv1 (Conv2DTranspose)    (None, 28, 28, 1)         801       \n",
      "=================================================================\n",
      "Total params: 275,979\n",
      "Trainable params: 275,979\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1_input (InputLayer)        (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 14, 14, 32)   832         conv1_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 7, 7, 64)     51264       conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 3, 3, 128)    73856       conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1152)         0           conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Dense)               (None, 10)           11530       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 10)           0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1152)         12672       lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 3, 3, 128)    0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "deconv3 (Conv2DTranspose)       (None, 7, 7, 64)     73792       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "deconv2 (Conv2DTranspose)       (None, 14, 14, 32)   51232       deconv3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "clustering (ClusteringLayer)    (None, 10)           100         embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "deconv1 (Conv2DTranspose)       (None, 28, 28, 1)    801         deconv2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 276,079\n",
      "Trainable params: 276,079\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'results/temp'\n",
    "\n",
    "import os\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "n_clusters = 10\n",
    "dcec = DCEC(input_shape=x.shape[1:], filters=[32, 64, 128, 10], n_clusters=n_clusters)\n",
    "dcec.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = 'adam'\n",
    "gamma = 0.1\n",
    "dcec.compile(loss=['kld', 'mse'], loss_weights=[gamma, 1], optimizer=optimizer)\n",
    "#dcec.fit(x, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update interval 140\n",
      "Save interval 1367.1875\n",
      "...pretraining CAE using default hyper-parameters:\n",
      "   optimizer='adam';   epochs=200\n",
      "...Pretraining...\n",
      "Epoch 1/200\n",
      "70000/70000 [==============================] - 55s 782us/step - loss: 0.0409\n",
      "Epoch 2/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0203\n",
      "Epoch 3/200\n",
      "70000/70000 [==============================] - 54s 776us/step - loss: 0.0175\n",
      "Epoch 4/200\n",
      "70000/70000 [==============================] - 55s 779us/step - loss: 0.0161\n",
      "Epoch 5/200\n",
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0152\n",
      "Epoch 6/200\n",
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0146\n",
      "Epoch 7/200\n",
      "70000/70000 [==============================] - 54s 771us/step - loss: 0.0141\n",
      "Epoch 8/200\n",
      "70000/70000 [==============================] - 54s 772us/step - loss: 0.0137\n",
      "Epoch 9/200\n",
      "70000/70000 [==============================] - 54s 772us/step - loss: 0.0134\n",
      "Epoch 10/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0131\n",
      "Epoch 11/200\n",
      "70000/70000 [==============================] - 54s 770us/step - loss: 0.0129\n",
      "Epoch 12/200\n",
      "70000/70000 [==============================] - 54s 771us/step - loss: 0.0126\n",
      "Epoch 13/200\n",
      "70000/70000 [==============================] - 54s 770us/step - loss: 0.0124\n",
      "Epoch 14/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0123\n",
      "Epoch 15/200\n",
      "70000/70000 [==============================] - 54s 772us/step - loss: 0.0121\n",
      "Epoch 16/200\n",
      "70000/70000 [==============================] - 54s 772us/step - loss: 0.0120\n",
      "Epoch 17/200\n",
      "70000/70000 [==============================] - 54s 772us/step - loss: 0.0119\n",
      "Epoch 18/200\n",
      "70000/70000 [==============================] - 54s 772us/step - loss: 0.0118\n",
      "Epoch 19/200\n",
      "70000/70000 [==============================] - 54s 771us/step - loss: 0.0117\n",
      "Epoch 20/200\n",
      "70000/70000 [==============================] - 54s 771us/step - loss: 0.0116\n",
      "Epoch 21/200\n",
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0115\n",
      "Epoch 22/200\n",
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0114\n",
      "Epoch 23/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0113\n",
      "Epoch 24/200\n",
      "70000/70000 [==============================] - 54s 771us/step - loss: 0.0112\n",
      "Epoch 25/200\n",
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0112\n",
      "Epoch 26/200\n",
      "70000/70000 [==============================] - 54s 770us/step - loss: 0.0111\n",
      "Epoch 27/200\n",
      "70000/70000 [==============================] - 55s 782us/step - loss: 0.0111\n",
      "Epoch 28/200\n",
      "70000/70000 [==============================] - 54s 775us/step - loss: 0.0110\n",
      "Epoch 29/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0109\n",
      "Epoch 30/200\n",
      "70000/70000 [==============================] - 55s 786us/step - loss: 0.0109\n",
      "Epoch 31/200\n",
      "70000/70000 [==============================] - 55s 781us/step - loss: 0.0108\n",
      "Epoch 32/200\n",
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0108\n",
      "Epoch 33/200\n",
      "70000/70000 [==============================] - 54s 772us/step - loss: 0.0107\n",
      "Epoch 34/200\n",
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0107\n",
      "Epoch 35/200\n",
      "70000/70000 [==============================] - 55s 779us/step - loss: 0.0107\n",
      "Epoch 36/200\n",
      "70000/70000 [==============================] - 54s 778us/step - loss: 0.0106\n",
      "Epoch 37/200\n",
      "70000/70000 [==============================] - 54s 778us/step - loss: 0.0106\n",
      "Epoch 38/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0105\n",
      "Epoch 39/200\n",
      "70000/70000 [==============================] - 54s 771us/step - loss: 0.0105\n",
      "Epoch 40/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0105\n",
      "Epoch 41/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0104\n",
      "Epoch 42/200\n",
      "70000/70000 [==============================] - 54s 772us/step - loss: 0.0104\n",
      "Epoch 43/200\n",
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0104\n",
      "Epoch 44/200\n",
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0104\n",
      "Epoch 45/200\n",
      "70000/70000 [==============================] - 54s 778us/step - loss: 0.0103\n",
      "Epoch 46/200\n",
      "70000/70000 [==============================] - 55s 780us/step - loss: 0.0103\n",
      "Epoch 47/200\n",
      "70000/70000 [==============================] - 54s 775us/step - loss: 0.0103\n",
      "Epoch 48/200\n",
      "70000/70000 [==============================] - 55s 781us/step - loss: 0.0102\n",
      "Epoch 49/200\n",
      "70000/70000 [==============================] - 54s 776us/step - loss: 0.0102\n",
      "Epoch 50/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0102\n",
      "Epoch 51/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0102\n",
      "Epoch 52/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0102\n",
      "Epoch 53/200\n",
      "70000/70000 [==============================] - 54s 778us/step - loss: 0.0101\n",
      "Epoch 54/200\n",
      "70000/70000 [==============================] - 54s 775us/step - loss: 0.0101\n",
      "Epoch 55/200\n",
      "70000/70000 [==============================] - 54s 775us/step - loss: 0.0101\n",
      "Epoch 56/200\n",
      "70000/70000 [==============================] - 55s 779us/step - loss: 0.0101\n",
      "Epoch 57/200\n",
      "70000/70000 [==============================] - 55s 779us/step - loss: 0.0100\n",
      "Epoch 58/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0100\n",
      "Epoch 59/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0100\n",
      "Epoch 60/200\n",
      "70000/70000 [==============================] - 56s 800us/step - loss: 0.0100\n",
      "Epoch 61/200\n",
      "70000/70000 [==============================] - 55s 786us/step - loss: 0.0100\n",
      "Epoch 62/200\n",
      "70000/70000 [==============================] - 54s 777us/step - loss: 0.0099\n",
      "Epoch 63/200\n",
      "70000/70000 [==============================] - 54s 776us/step - loss: 0.0099\n",
      "Epoch 64/200\n",
      "70000/70000 [==============================] - 54s 772us/step - loss: 0.0099\n",
      "Epoch 65/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0099\n",
      "Epoch 66/200\n",
      "70000/70000 [==============================] - 54s 777us/step - loss: 0.0099\n",
      "Epoch 67/200\n",
      "70000/70000 [==============================] - 54s 772us/step - loss: 0.0099\n",
      "Epoch 68/200\n",
      "70000/70000 [==============================] - 54s 771us/step - loss: 0.0098\n",
      "Epoch 69/200\n",
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0098\n",
      "Epoch 70/200\n",
      "70000/70000 [==============================] - 56s 793us/step - loss: 0.0098\n",
      "Epoch 71/200\n",
      "70000/70000 [==============================] - 55s 786us/step - loss: 0.0098\n",
      "Epoch 72/200\n",
      "70000/70000 [==============================] - 56s 793us/step - loss: 0.0098\n",
      "Epoch 73/200\n",
      "70000/70000 [==============================] - 55s 785us/step - loss: 0.0098\n",
      "Epoch 74/200\n",
      "70000/70000 [==============================] - 55s 782us/step - loss: 0.0098\n",
      "Epoch 75/200\n",
      "70000/70000 [==============================] - 54s 776us/step - loss: 0.0097\n",
      "Epoch 76/200\n",
      "70000/70000 [==============================] - 54s 775us/step - loss: 0.0097\n",
      "Epoch 77/200\n",
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0097\n",
      "Epoch 78/200\n",
      "70000/70000 [==============================] - 54s 775us/step - loss: 0.0097\n",
      "Epoch 79/200\n",
      "70000/70000 [==============================] - 54s 775us/step - loss: 0.0097\n",
      "Epoch 80/200\n",
      "70000/70000 [==============================] - 55s 783us/step - loss: 0.0097\n",
      "Epoch 81/200\n",
      "70000/70000 [==============================] - 55s 781us/step - loss: 0.0097\n",
      "Epoch 82/200\n",
      "70000/70000 [==============================] - 54s 776us/step - loss: 0.0097\n",
      "Epoch 83/200\n",
      "70000/70000 [==============================] - 54s 775us/step - loss: 0.0096\n",
      "Epoch 84/200\n",
      "70000/70000 [==============================] - 54s 776us/step - loss: 0.0096\n",
      "Epoch 85/200\n",
      "70000/70000 [==============================] - 54s 775us/step - loss: 0.00962s -\n",
      "Epoch 86/200\n",
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0096\n",
      "Epoch 87/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0096\n",
      "Epoch 88/200\n",
      "70000/70000 [==============================] - 54s 776us/step - loss: 0.0096\n",
      "Epoch 89/200\n",
      "70000/70000 [==============================] - 54s 776us/step - loss: 0.0096\n",
      "Epoch 90/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0096\n",
      "Epoch 91/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0096\n",
      "Epoch 92/200\n",
      "70000/70000 [==============================] - 54s 771us/step - loss: 0.0096\n",
      "Epoch 93/200\n",
      "70000/70000 [==============================] - 54s 776us/step - loss: 0.0095\n",
      "Epoch 94/200\n",
      "70000/70000 [==============================] - 54s 778us/step - loss: 0.0095\n",
      "Epoch 95/200\n",
      "70000/70000 [==============================] - 54s 776us/step - loss: 0.0095\n",
      "Epoch 96/200\n",
      "70000/70000 [==============================] - 54s 778us/step - loss: 0.0095\n",
      "Epoch 97/200\n",
      "70000/70000 [==============================] - 55s 784us/step - loss: 0.0095\n",
      "Epoch 98/200\n",
      "70000/70000 [==============================] - 56s 806us/step - loss: 0.0095\n",
      "Epoch 99/200\n",
      "70000/70000 [==============================] - 57s 816us/step - loss: 0.0095\n",
      "Epoch 100/200\n",
      "70000/70000 [==============================] - 56s 799us/step - loss: 0.0095\n",
      "Epoch 101/200\n",
      "70000/70000 [==============================] - 55s 788us/step - loss: 0.0095\n",
      "Epoch 102/200\n",
      "70000/70000 [==============================] - 54s 777us/step - loss: 0.0095\n",
      "Epoch 103/200\n",
      "70000/70000 [==============================] - 54s 776us/step - loss: 0.0094\n",
      "Epoch 104/200\n",
      "70000/70000 [==============================] - 54s 778us/step - loss: 0.0094\n",
      "Epoch 105/200\n",
      "70000/70000 [==============================] - 55s 779us/step - loss: 0.0094\n",
      "Epoch 106/200\n",
      "70000/70000 [==============================] - 55s 781us/step - loss: 0.0094\n",
      "Epoch 107/200\n",
      "70000/70000 [==============================] - 54s 777us/step - loss: 0.0094\n",
      "Epoch 108/200\n",
      "70000/70000 [==============================] - 54s 775us/step - loss: 0.0094\n",
      "Epoch 109/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0094\n",
      "Epoch 110/200\n",
      "70000/70000 [==============================] - 55s 779us/step - loss: 0.0094\n",
      "Epoch 111/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0094\n",
      "Epoch 112/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0094\n",
      "Epoch 113/200\n",
      "70000/70000 [==============================] - 54s 776us/step - loss: 0.0094\n",
      "Epoch 114/200\n",
      "70000/70000 [==============================] - 54s 776us/step - loss: 0.0094\n",
      "Epoch 115/200\n",
      "70000/70000 [==============================] - 54s 778us/step - loss: 0.0094\n",
      "Epoch 116/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0093\n",
      "Epoch 117/200\n",
      "70000/70000 [==============================] - 54s 776us/step - loss: 0.0093\n",
      "Epoch 118/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.00931s - loss\n",
      "Epoch 119/200\n",
      "70000/70000 [==============================] - 54s 775us/step - loss: 0.0093\n",
      "Epoch 120/200\n",
      "70000/70000 [==============================] - 54s 777us/step - loss: 0.0093\n",
      "Epoch 121/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0093\n",
      "Epoch 122/200\n",
      "70000/70000 [==============================] - 54s 778us/step - loss: 0.0093\n",
      "Epoch 123/200\n",
      "70000/70000 [==============================] - 55s 780us/step - loss: 0.0093\n",
      "Epoch 124/200\n",
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0093\n",
      "Epoch 125/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0093\n",
      "Epoch 126/200\n",
      "70000/70000 [==============================] - 54s 777us/step - loss: 0.0093\n",
      "Epoch 127/200\n",
      "70000/70000 [==============================] - 54s 776us/step - loss: 0.0093\n",
      "Epoch 128/200\n",
      "70000/70000 [==============================] - 54s 777us/step - loss: 0.0093\n",
      "Epoch 129/200\n",
      "70000/70000 [==============================] - 54s 777us/step - loss: 0.0093\n",
      "Epoch 130/200\n",
      "70000/70000 [==============================] - 54s 777us/step - loss: 0.0093\n",
      "Epoch 131/200\n",
      "70000/70000 [==============================] - 54s 772us/step - loss: 0.0093\n",
      "Epoch 132/200\n",
      "70000/70000 [==============================] - 54s 777us/step - loss: 0.0092\n",
      "Epoch 133/200\n",
      "70000/70000 [==============================] - 54s 775us/step - loss: 0.0092\n",
      "Epoch 134/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0092\n",
      "Epoch 135/200\n",
      "70000/70000 [==============================] - 54s 772us/step - loss: 0.0092\n",
      "Epoch 136/200\n",
      "70000/70000 [==============================] - 54s 771us/step - loss: 0.0092\n",
      "Epoch 137/200\n",
      "70000/70000 [==============================] - 54s 772us/step - loss: 0.0092\n",
      "Epoch 138/200\n",
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0092\n",
      "Epoch 139/200\n",
      "70000/70000 [==============================] - 55s 779us/step - loss: 0.0092\n",
      "Epoch 140/200\n",
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0092\n",
      "Epoch 141/200\n",
      "70000/70000 [==============================] - 54s 771us/step - loss: 0.0092\n",
      "Epoch 142/200\n",
      "70000/70000 [==============================] - 54s 771us/step - loss: 0.0092\n",
      "Epoch 143/200\n",
      "70000/70000 [==============================] - 54s 771us/step - loss: 0.0092\n",
      "Epoch 144/200\n",
      "70000/70000 [==============================] - 54s 772us/step - loss: 0.0092\n",
      "Epoch 145/200\n",
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0092\n",
      "Epoch 146/200\n",
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0092\n",
      "Epoch 147/200\n",
      "70000/70000 [==============================] - 54s 775us/step - loss: 0.0092\n",
      "Epoch 148/200\n",
      "70000/70000 [==============================] - 54s 772us/step - loss: 0.0091\n",
      "Epoch 149/200\n",
      "70000/70000 [==============================] - 55s 782us/step - loss: 0.0091\n",
      "Epoch 150/200\n",
      "70000/70000 [==============================] - 55s 790us/step - loss: 0.0091\n",
      "Epoch 151/200\n",
      "70000/70000 [==============================] - 55s 783us/step - loss: 0.0091\n",
      "Epoch 152/200\n",
      "70000/70000 [==============================] - 54s 778us/step - loss: 0.0091\n",
      "Epoch 153/200\n",
      "70000/70000 [==============================] - 55s 782us/step - loss: 0.0091\n",
      "Epoch 154/200\n",
      "70000/70000 [==============================] - 55s 783us/step - loss: 0.0091\n",
      "Epoch 155/200\n",
      "70000/70000 [==============================] - 55s 784us/step - loss: 0.0091\n",
      "Epoch 156/200\n",
      "70000/70000 [==============================] - 55s 785us/step - loss: 0.0091\n",
      "Epoch 157/200\n",
      "70000/70000 [==============================] - 56s 802us/step - loss: 0.0091\n",
      "Epoch 158/200\n",
      "70000/70000 [==============================] - 54s 776us/step - loss: 0.0091\n",
      "Epoch 159/200\n",
      "70000/70000 [==============================] - 55s 780us/step - loss: 0.0091\n",
      "Epoch 160/200\n",
      "70000/70000 [==============================] - 56s 799us/step - loss: 0.0091\n",
      "Epoch 161/200\n",
      "70000/70000 [==============================] - 57s 809us/step - loss: 0.0091\n",
      "Epoch 162/200\n",
      "70000/70000 [==============================] - 56s 805us/step - loss: 0.0091\n",
      "Epoch 163/200\n",
      "70000/70000 [==============================] - 56s 804us/step - loss: 0.0091\n",
      "Epoch 164/200\n",
      "70000/70000 [==============================] - 55s 789us/step - loss: 0.0091\n",
      "Epoch 165/200\n",
      "70000/70000 [==============================] - 55s 790us/step - loss: 0.0091\n",
      "Epoch 166/200\n",
      "70000/70000 [==============================] - 56s 793us/step - loss: 0.0091\n",
      "Epoch 167/200\n",
      "70000/70000 [==============================] - 55s 784us/step - loss: 0.0091\n",
      "Epoch 168/200\n",
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0091\n",
      "Epoch 169/200\n",
      "70000/70000 [==============================] - 54s 771us/step - loss: 0.0091\n",
      "Epoch 170/200\n",
      "70000/70000 [==============================] - 54s 771us/step - loss: 0.0091\n",
      "Epoch 171/200\n",
      "70000/70000 [==============================] - 54s 776us/step - loss: 0.0090\n",
      "Epoch 172/200\n",
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0091\n",
      "Epoch 173/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0090\n",
      "Epoch 174/200\n",
      "70000/70000 [==============================] - 54s 772us/step - loss: 0.0090\n",
      "Epoch 175/200\n",
      "70000/70000 [==============================] - 54s 770us/step - loss: 0.0090\n",
      "Epoch 176/200\n",
      "70000/70000 [==============================] - 54s 772us/step - loss: 0.0090\n",
      "Epoch 177/200\n",
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0090\n",
      "Epoch 178/200\n",
      "70000/70000 [==============================] - 54s 771us/step - loss: 0.0090\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0090\n",
      "Epoch 180/200\n",
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0090\n",
      "Epoch 181/200\n",
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0090\n",
      "Epoch 182/200\n",
      "70000/70000 [==============================] - 54s 777us/step - loss: 0.0090\n",
      "Epoch 183/200\n",
      "70000/70000 [==============================] - 54s 772us/step - loss: 0.0090\n",
      "Epoch 184/200\n",
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0090\n",
      "Epoch 185/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0090\n",
      "Epoch 186/200\n",
      "70000/70000 [==============================] - 54s 772us/step - loss: 0.0090\n",
      "Epoch 187/200\n",
      "70000/70000 [==============================] - 54s 771us/step - loss: 0.0090\n",
      "Epoch 188/200\n",
      "70000/70000 [==============================] - 54s 772us/step - loss: 0.0090\n",
      "Epoch 189/200\n",
      "70000/70000 [==============================] - 54s 770us/step - loss: 0.0090\n",
      "Epoch 190/200\n",
      "70000/70000 [==============================] - 54s 770us/step - loss: 0.0090\n",
      "Epoch 191/200\n",
      "70000/70000 [==============================] - 54s 770us/step - loss: 0.0090\n",
      "Epoch 192/200\n",
      "70000/70000 [==============================] - 54s 771us/step - loss: 0.0090\n",
      "Epoch 193/200\n",
      "70000/70000 [==============================] - 54s 772us/step - loss: 0.0090\n",
      "Epoch 194/200\n",
      "70000/70000 [==============================] - 54s 774us/step - loss: 0.0090\n",
      "Epoch 195/200\n",
      "70000/70000 [==============================] - 54s 772us/step - loss: 0.0090\n",
      "Epoch 196/200\n",
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0089\n",
      "Epoch 197/200\n",
      "70000/70000 [==============================] - 54s 771us/step - loss: 0.0090\n",
      "Epoch 198/200\n",
      "70000/70000 [==============================] - 54s 770us/step - loss: 0.0090\n",
      "Epoch 199/200\n",
      "70000/70000 [==============================] - 54s 772us/step - loss: 0.0090\n",
      "Epoch 200/200\n",
      "70000/70000 [==============================] - 54s 773us/step - loss: 0.0089\n",
      "Pretrained weights are saved to ./results/temp/pretrain_cae_model.h5\n",
      "Initializing cluster centers with k-means.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stf4041\\Documents\\DownloadedSoftware\\envs\\py36\\lib\\site-packages\\sklearn\\metrics\\cluster\\supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 : Acc 0.9445 , nmi 0.87651 , ari 0.88077 ; loss= [0 0 0]\n",
      "saving model to: ./results/temp/dcec_model_0.h5\n",
      "Iter 140 : Acc 0.95761 , nmi 0.90204 , ari 0.90871 ; loss= [0.01525 0.03935 0.01131]\n",
      "Iter 280 : Acc 0.96591 , nmi 0.91762 , ari 0.92618 ; loss= [0.02467 0.08691 0.01598]\n",
      "Iter 420 : Acc 0.97044 , nmi 0.92638 , ari 0.93578 ; loss= [0.03005 0.12107 0.01794]\n",
      "Iter 560 : Acc 0.9725 , nmi 0.93017 , ari 0.94016 ; loss= [0.02862 0.11328 0.01729]\n",
      "Iter 700 : Acc 0.9749 , nmi 0.93527 , ari 0.94533 ; loss= [0.02748 0.10276 0.01721]\n",
      "Iter 840 : Acc 0.97577 , nmi 0.93701 , ari 0.94719 ; loss= [0.025   0.09534 0.01547]\n",
      "Iter 980 : Acc 0.97663 , nmi 0.93899 , ari 0.94904 ; loss= [0.02485 0.09161 0.01569]\n",
      "Iter 1120 : Acc 0.97726 , nmi 0.94023 , ari 0.9504 ; loss= [0.02459 0.09079 0.01551]\n",
      "Iter 1260 : Acc 0.97754 , nmi 0.94091 , ari 0.951 ; loss= [0.02444 0.0889  0.01555]\n",
      "Iter 1400 : Acc 0.97789 , nmi 0.9417 , ari 0.95171 ; loss= [0.02421 0.08857 0.01536]\n",
      "Iter 1540 : Acc 0.97813 , nmi 0.94226 , ari 0.95222 ; loss= [0.02329 0.0741  0.01588]\n",
      "Iter 1680 : Acc 0.97846 , nmi 0.94295 , ari 0.95289 ; loss= [0.02236 0.07758 0.0146 ]\n",
      "Iter 1820 : Acc 0.97863 , nmi 0.94339 , ari 0.95327 ; loss= [0.02526 0.08643 0.01662]\n",
      "Iter 1960 : Acc 0.97861 , nmi 0.94334 , ari 0.95324 ; loss= [0.02115 0.06492 0.01466]\n",
      "Iter 2100 : Acc 0.9789 , nmi 0.94407 , ari 0.95386 ; loss= [0.02402 0.07592 0.01642]\n",
      "Iter 2240 : Acc 0.97906 , nmi 0.94439 , ari 0.95419 ; loss= [0.0227  0.08258 0.01444]\n",
      "Iter 2380 : Acc 0.97926 , nmi 0.94497 , ari 0.95463 ; loss= [0.02258 0.07288 0.0153 ]\n",
      "Iter 2520 : Acc 0.97917 , nmi 0.94478 , ari 0.95443 ; loss= [0.02324 0.07913 0.01532]\n",
      "Iter 2660 : Acc 0.97944 , nmi 0.94521 , ari 0.95502 ; loss= [0.02679 0.09951 0.01684]\n",
      "Iter 2800 : Acc 0.97963 , nmi 0.94566 , ari 0.95539 ; loss= [0.02211 0.07346 0.01476]\n",
      "Iter 2940 : Acc 0.97986 , nmi 0.94621 , ari 0.95587 ; loss= [0.01978 0.06462 0.01332]\n",
      "Iter 3080 : Acc 0.97993 , nmi 0.94651 , ari 0.95603 ; loss= [0.02239 0.06968 0.01543]\n",
      "Iter 3220 : Acc 0.97997 , nmi 0.94662 , ari 0.95611 ; loss= [0.02042 0.06549 0.01387]\n",
      "Iter 3360 : Acc 0.98047 , nmi 0.94759 , ari 0.95717 ; loss= [0.01966 0.06529 0.01314]\n",
      "Iter 3500 : Acc 0.9804 , nmi 0.94756 , ari 0.95702 ; loss= [0.02044 0.06119 0.01432]\n",
      "Iter 3640 : Acc 0.9804 , nmi 0.94749 , ari 0.95701 ; loss= [0.0211  0.06412 0.01469]\n",
      "Iter 3780 : Acc 0.98069 , nmi 0.94825 , ari 0.95764 ; loss= [0.02054 0.05714 0.01482]\n",
      "Iter 3920 : Acc 0.98053 , nmi 0.94789 , ari 0.9573 ; loss= [0.02048 0.0625  0.01423]\n",
      "Iter 4060 : Acc 0.9807 , nmi 0.94831 , ari 0.95765 ; loss= [0.01945 0.05569 0.01388]\n",
      "Iter 4200 : Acc 0.9807 , nmi 0.94827 , ari 0.95766 ; loss= [0.01988 0.05705 0.01418]\n",
      "Iter 4340 : Acc 0.98077 , nmi 0.94842 , ari 0.95778 ; loss= [0.01826 0.04806 0.01345]\n",
      "Iter 4480 : Acc 0.98086 , nmi 0.94873 , ari 0.95799 ; loss= [0.01958 0.05706 0.01387]\n",
      "Iter 4620 : Acc 0.98067 , nmi 0.94819 , ari 0.95758 ; loss= [0.02049 0.06498 0.01399]\n",
      "Iter 4760 : Acc 0.98081 , nmi 0.94877 , ari 0.95791 ; loss= [0.01997 0.06097 0.01387]\n",
      "Iter 4900 : Acc 0.98087 , nmi 0.94885 , ari 0.95802 ; loss= [0.01981 0.06336 0.01348]\n",
      "Iter 5040 : Acc 0.98084 , nmi 0.94872 , ari 0.95795 ; loss= [0.0197  0.05604 0.0141 ]\n",
      "Iter 5180 : Acc 0.98099 , nmi 0.94893 , ari 0.95827 ; loss= [0.01982 0.06595 0.01323]\n",
      "Iter 5320 : Acc 0.98079 , nmi 0.94875 , ari 0.95783 ; loss= [0.02155 0.06236 0.01531]\n",
      "Iter 5460 : Acc 0.98086 , nmi 0.9488 , ari 0.95798 ; loss= [0.01815 0.05472 0.01268]\n",
      "Iter 5600 : Acc 0.98103 , nmi 0.94928 , ari 0.95835 ; loss= [0.01903 0.05775 0.01326]\n",
      "Iter 5740 : Acc 0.981 , nmi 0.94911 , ari 0.95829 ; loss= [0.0178  0.04306 0.01349]\n",
      "Iter 5880 : Acc 0.98106 , nmi 0.94921 , ari 0.9584 ; loss= [0.01993 0.05791 0.01414]\n",
      "Iter 6020 : Acc 0.98101 , nmi 0.94914 , ari 0.95832 ; loss= [0.01851 0.05359 0.01315]\n",
      "Iter 6160 : Acc 0.98104 , nmi 0.94916 , ari 0.95837 ; loss= [0.01758 0.04606 0.01298]\n",
      "Iter 6300 : Acc 0.98094 , nmi 0.94901 , ari 0.95814 ; loss= [0.01916 0.04375 0.01478]\n",
      "Iter 6440 : Acc 0.98094 , nmi 0.94897 , ari 0.95815 ; loss= [0.01784 0.04792 0.01305]\n",
      "Iter 6580 : Acc 0.98099 , nmi 0.94931 , ari 0.95825 ; loss= [0.01834 0.05324 0.01301]\n",
      "Iter 6720 : Acc 0.981 , nmi 0.94919 , ari 0.95827 ; loss= [0.01804 0.04614 0.01342]\n",
      "Iter 6860 : Acc 0.98096 , nmi 0.94924 , ari 0.95819 ; loss= [0.01681 0.04437 0.01237]\n",
      "Iter 7000 : Acc 0.98104 , nmi 0.94927 , ari 0.95837 ; loss= [0.02039 0.05204 0.01518]\n",
      "Iter 7140 : Acc 0.98111 , nmi 0.94936 , ari 0.95851 ; loss= [0.01771 0.04519 0.01319]\n",
      "Iter 7280 : Acc 0.98097 , nmi 0.94908 , ari 0.95823 ; loss= [0.01968 0.05812 0.01387]\n",
      "Iter 7420 : Acc 0.98116 , nmi 0.94955 , ari 0.95861 ; loss= [0.01712 0.04142 0.01298]\n",
      "Iter 7560 : Acc 0.98109 , nmi 0.94935 , ari 0.95847 ; loss= [0.02125 0.05855 0.0154 ]\n",
      "delta_label  0.0009714285714285714 < tol  0.001\n",
      "Reached tolerance threshold. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "dcec.fit(x, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "met_nmi = normalized_mutual_info_score\n",
    "met_ari = adjusted_rand_score\n",
    "\n",
    "\n",
    "def accu(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy. Require scikit-learn installed\n",
    "    # Arguments\n",
    "        y: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dcec.y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9810857142857143\n"
     ]
    }
   ],
   "source": [
    "print('acc:', accu(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeData(Z, labels, num_clusters, title):\n",
    "    '''TSNE visualization of the points in latent space Z\n",
    "    :param Z: Numpy array containing points in latent space in which clustering was performed\n",
    "    :param labels: True labels - used for coloring points\n",
    "    :param num_clusters: Total number of clusters\n",
    "    :param title: filename where the plot should be saved\n",
    "    :return: None - (side effect) saves clustering visualization plot in specified location'''\n",
    "    labels = labels.astype(int)\n",
    "    tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "    #Z_tsne = np.squeeze(tsne.fit_transform(Z.reshape(-1,1))) #Mleen added squeeze\n",
    "    Z_tsne = tsne.fit_transform(Z)\n",
    "    print (Z_tsne)\n",
    "    #print (Z_tsne[:, 1])\n",
    "    fig = plt.figure()\n",
    "    plt.scatter(Z_tsne[:, 0], Z_tsne[:, 1], s=2, c=labels, cmap=plt.cm.get_cmap(\"jet\", num_clusters))\n",
    "    plt.colorbar(ticks=range(num_clusters))\n",
    "    fig.savefig(title, dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = dcec.encoder.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-30.372486   -0.7733181]\n",
      " [-15.640977   28.228704 ]\n",
      " [ 53.633972    1.2883842]\n",
      " ...\n",
      " [ 48.292187   11.564295 ]\n",
      " [-53.8221      1.8966876]\n",
      " [ -0.6341051  44.0418   ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualizeData(encoded, y_pred, n_clusters, \"autoencoder.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
